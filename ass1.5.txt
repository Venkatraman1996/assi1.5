1) HADOOP is a open source tool which consists of a set of programs to carry on big data operations.
the storage of data is carried out using HDFS and processing is carried out by map reduce.

=====================================================================================================================================

2) components of hadoop framework:

1) Hadoop Common-Apache Foundation has pre-defined set of utilities and libraries that can be used by other modules within
                 the Hadoop ecosystem. For example, if HBase and Hive want to access HDFS they need to make of Java archives
                  that are stored in Hadoop Common.
2)HDFS-Distributed storage system.
3)Map reduce-distributed processing system
4)YARN-it is used to enable dynamic utilization of resources.


===========================================================================================================================================

3) Reasons to learn big data technologies:
                                       there is an increasing demand for data analysts as the production of data has exponentially
increased over the recent years as the data is not only been generated by the business transactions by companies but also by common
people from their gadgets such as mobile phones,cameras,sensors,etc. since these data can not be discarded and has to me managed
the big data technology has a  huge potential.

==============================================================================================================================================
